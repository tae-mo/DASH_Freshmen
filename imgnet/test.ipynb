{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/home/data/Imagenet')\n",
    "train_path = root / 'train'\n",
    "val_path = root / 'val'\n",
    "label_path = root / 'label'\n",
    "\n",
    "def ImgNetData(train_path, val_path, distributed=True):\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomChoice([transforms.Resize(256), transforms.Resize(480)]),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        \n",
    "        ### 이걸로 사용해볼 순 없나?\n",
    "        # transforms.RandomResizedCrop((224,224)),\n",
    "        # transforms.Resize((256,256)),\n",
    "        # transforms.CenterCrop((224,224)),\n",
    "        \n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "    ])\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(244),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    train_data = ImageFolder(root=train_path, transform=train_transforms)\n",
    "    val_data = ImageFolder(root=val_path, transform=val_transforms)\n",
    "    \n",
    "    return train_data, val_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n01440764': 'tench',\n",
       " 'n01443537': 'goldfish',\n",
       " 'n01484850': 'great',\n",
       " 'n01491361': 'tiger',\n",
       " 'n01494475': 'hammerhead',\n",
       " 'n01496331': 'electric',\n",
       " 'n01498041': 'stingray',\n",
       " 'n01514668': 'cock',\n",
       " 'n01514859': 'hen',\n",
       " 'n01518878': 'ostrich',\n",
       " 'n01530575': 'brambling',\n",
       " 'n01531178': 'goldfinch',\n",
       " 'n01532829': 'house',\n",
       " 'n01534433': 'junco',\n",
       " 'n01537544': 'indigo',\n",
       " 'n01558993': 'robin',\n",
       " 'n01560419': 'bulbul',\n",
       " 'n01580077': 'jay',\n",
       " 'n01582220': 'magpie',\n",
       " 'n01592084': 'chickadee',\n",
       " 'n01601694': 'water',\n",
       " 'n01608432': 'kite',\n",
       " 'n01614925': 'bald',\n",
       " 'n01616318': 'vulture',\n",
       " 'n01622779': 'great',\n",
       " 'n01629819': 'European',\n",
       " 'n01630670': 'common',\n",
       " 'n01631663': 'eft',\n",
       " 'n01632458': 'spotted',\n",
       " 'n01632777': 'axolotl',\n",
       " 'n01641577': 'bullfrog',\n",
       " 'n01644373': 'tree',\n",
       " 'n01644900': 'tailed',\n",
       " 'n01664065': 'loggerhead',\n",
       " 'n01665541': 'leatherback',\n",
       " 'n01667114': 'mud',\n",
       " 'n01667778': 'terrapin',\n",
       " 'n01669191': 'box',\n",
       " 'n01675722': 'banded',\n",
       " 'n01677366': 'common',\n",
       " 'n01682714': 'American',\n",
       " 'n01685808': 'whiptail',\n",
       " 'n01687978': 'agama',\n",
       " 'n01688243': 'frilled',\n",
       " 'n01689811': 'alligator',\n",
       " 'n01692333': 'Gila',\n",
       " 'n01693334': 'green',\n",
       " 'n01694178': 'African',\n",
       " 'n01695060': 'Komodo',\n",
       " 'n01697457': 'African',\n",
       " 'n01698640': 'American',\n",
       " 'n01704323': 'triceratops',\n",
       " 'n01728572': 'thunder',\n",
       " 'n01728920': 'ringneck',\n",
       " 'n01729322': 'hognose',\n",
       " 'n01729977': 'green',\n",
       " 'n01734418': 'king',\n",
       " 'n01735189': 'garter',\n",
       " 'n01737021': 'water',\n",
       " 'n01739381': 'vine',\n",
       " 'n01740131': 'night',\n",
       " 'n01742172': 'boa',\n",
       " 'n01744401': 'rock',\n",
       " 'n01748264': 'Indian',\n",
       " 'n01749939': 'green',\n",
       " 'n01751748': 'sea',\n",
       " 'n01753488': 'horned',\n",
       " 'n01755581': 'diamondback',\n",
       " 'n01756291': 'sidewinder',\n",
       " 'n01768244': 'trilobite',\n",
       " 'n01770081': 'harvestman',\n",
       " 'n01770393': 'scorpion',\n",
       " 'n01773157': 'black',\n",
       " 'n01773549': 'barn',\n",
       " 'n01773797': 'garden',\n",
       " 'n01774384': 'black',\n",
       " 'n01774750': 'tarantula',\n",
       " 'n01775062': 'wolf',\n",
       " 'n01776313': 'tick',\n",
       " 'n01784675': 'centipede',\n",
       " 'n01795545': 'black',\n",
       " 'n01796340': 'ptarmigan',\n",
       " 'n01797886': 'ruffed',\n",
       " 'n01798484': 'prairie',\n",
       " 'n01806143': 'peacock',\n",
       " 'n01806567': 'quail',\n",
       " 'n01807496': 'partridge',\n",
       " 'n01817953': 'African',\n",
       " 'n01818515': 'macaw',\n",
       " 'n01819313': 'sulphurcrested',\n",
       " 'n01820546': 'lorikeet',\n",
       " 'n01824575': 'coucal',\n",
       " 'n01828970': 'bee',\n",
       " 'n01829413': 'hornbill',\n",
       " 'n01833805': 'hummingbird',\n",
       " 'n01843065': 'jacamar',\n",
       " 'n01843383': 'toucan',\n",
       " 'n01847000': 'drake',\n",
       " 'n01855032': 'redbreasted',\n",
       " 'n01855672': 'goose',\n",
       " 'n01860187': 'black',\n",
       " 'n01871265': 'tusker',\n",
       " 'n01872401': 'echidna',\n",
       " 'n01873310': 'platypus',\n",
       " 'n01877812': 'wallaby',\n",
       " 'n01882714': 'koala',\n",
       " 'n01883070': 'wombat',\n",
       " 'n01910747': 'jellyfish',\n",
       " 'n01914609': 'sea',\n",
       " 'n01917289': 'brain',\n",
       " 'n01924916': 'flatworm',\n",
       " 'n01930112': 'nematode',\n",
       " 'n01943899': 'conch',\n",
       " 'n01944390': 'snail',\n",
       " 'n01945685': 'slug',\n",
       " 'n01950731': 'sea',\n",
       " 'n01955084': 'chiton',\n",
       " 'n01968897': 'chambered',\n",
       " 'n01978287': 'Dungeness',\n",
       " 'n01978455': 'rock',\n",
       " 'n01980166': 'fiddler',\n",
       " 'n01981276': 'king',\n",
       " 'n01983481': 'American',\n",
       " 'n01984695': 'spiny',\n",
       " 'n01985128': 'crayfish',\n",
       " 'n01986214': 'hermit',\n",
       " 'n01990800': 'isopod',\n",
       " 'n02002556': 'white',\n",
       " 'n02002724': 'black',\n",
       " 'n02006656': 'spoonbill',\n",
       " 'n02007558': 'flamingo',\n",
       " 'n02009229': 'little',\n",
       " 'n02009912': 'American',\n",
       " 'n02011460': 'bittern',\n",
       " 'n02012849': 'crane',\n",
       " 'n02013706': 'limpkin',\n",
       " 'n02017213': 'European',\n",
       " 'n02018207': 'American',\n",
       " 'n02018795': 'bustard',\n",
       " 'n02025239': 'ruddy',\n",
       " 'n02027492': 'redbacked',\n",
       " 'n02028035': 'redshank',\n",
       " 'n02033041': 'dowitcher',\n",
       " 'n02037110': 'oystercatcher',\n",
       " 'n02051845': 'pelican',\n",
       " 'n02056570': 'king',\n",
       " 'n02058221': 'albatross',\n",
       " 'n02066245': 'grey',\n",
       " 'n02071294': 'killer',\n",
       " 'n02074367': 'dugong',\n",
       " 'n02077923': 'sea',\n",
       " 'n02085620': 'Chihuahua',\n",
       " 'n02085782': 'Japanese',\n",
       " 'n02085936': 'Maltese',\n",
       " 'n02086079': 'Pekinese',\n",
       " 'n02086240': 'ShihTzu',\n",
       " 'n02086646': 'Blenheim',\n",
       " 'n02086910': 'papillon',\n",
       " 'n02087046': 'toy',\n",
       " 'n02087394': 'Rhodesian',\n",
       " 'n02088094': 'Afghan',\n",
       " 'n02088238': 'basset',\n",
       " 'n02088364': 'beagle',\n",
       " 'n02088466': 'bloodhound',\n",
       " 'n02088632': 'bluetick',\n",
       " 'n02089078': 'blackandtan',\n",
       " 'n02089867': 'Walker',\n",
       " 'n02089973': 'English',\n",
       " 'n02090379': 'redbone',\n",
       " 'n02090622': 'borzoi',\n",
       " 'n02090721': 'Irish',\n",
       " 'n02091032': 'Italian',\n",
       " 'n02091134': 'whippet',\n",
       " 'n02091244': 'Ibizan',\n",
       " 'n02091467': 'Norwegian',\n",
       " 'n02091635': 'otterhound',\n",
       " 'n02091831': 'Saluki',\n",
       " 'n02092002': 'Scottish',\n",
       " 'n02092339': 'Weimaraner',\n",
       " 'n02093256': 'Staffordshire',\n",
       " 'n02093428': 'American',\n",
       " 'n02093647': 'Bedlington',\n",
       " 'n02093754': 'Border',\n",
       " 'n02093859': 'Kerry',\n",
       " 'n02093991': 'Irish',\n",
       " 'n02094114': 'Norfolk',\n",
       " 'n02094258': 'Norwich',\n",
       " 'n02094433': 'Yorkshire',\n",
       " 'n02095314': 'wirehaired',\n",
       " 'n02095570': 'Lakeland',\n",
       " 'n02095889': 'Sealyham',\n",
       " 'n02096051': 'Airedale',\n",
       " 'n02096177': 'cairn',\n",
       " 'n02096294': 'Australian',\n",
       " 'n02096437': 'Dandie',\n",
       " 'n02096585': 'Boston',\n",
       " 'n02097047': 'miniature',\n",
       " 'n02097130': 'giant',\n",
       " 'n02097209': 'standard',\n",
       " 'n02097298': 'Scotch',\n",
       " 'n02097474': 'Tibetan',\n",
       " 'n02097658': 'silky',\n",
       " 'n02098105': 'softcoated',\n",
       " 'n02098286': 'West',\n",
       " 'n02098413': 'Lhasa',\n",
       " 'n02099267': 'flatcoated',\n",
       " 'n02099429': 'curlycoated',\n",
       " 'n02099601': 'golden',\n",
       " 'n02099712': 'Labrador',\n",
       " 'n02099849': 'Chesapeake',\n",
       " 'n02100236': 'German',\n",
       " 'n02100583': 'vizsla',\n",
       " 'n02100735': 'English',\n",
       " 'n02100877': 'Irish',\n",
       " 'n02101006': 'Gordon',\n",
       " 'n02101388': 'Brittany',\n",
       " 'n02101556': 'clumber',\n",
       " 'n02102040': 'English',\n",
       " 'n02102177': 'Welsh',\n",
       " 'n02102318': 'cocker',\n",
       " 'n02102480': 'Sussex',\n",
       " 'n02102973': 'Irish',\n",
       " 'n02104029': 'kuvasz',\n",
       " 'n02104365': 'schipperke',\n",
       " 'n02105056': 'groenendael',\n",
       " 'n02105162': 'malinois',\n",
       " 'n02105251': 'briard',\n",
       " 'n02105412': 'kelpie',\n",
       " 'n02105505': 'komondor',\n",
       " 'n02105641': 'Old',\n",
       " 'n02105855': 'Shetland',\n",
       " 'n02106030': 'collie',\n",
       " 'n02106166': 'Border',\n",
       " 'n02106382': 'Bouvier',\n",
       " 'n02106550': 'Rottweiler',\n",
       " 'n02106662': 'German',\n",
       " 'n02107142': 'Doberman',\n",
       " 'n02107312': 'miniature',\n",
       " 'n02107574': 'Greater',\n",
       " 'n02107683': 'Bernese',\n",
       " 'n02107908': 'Appenzeller',\n",
       " 'n02108000': 'EntleBucher',\n",
       " 'n02108089': 'boxer',\n",
       " 'n02108422': 'bull',\n",
       " 'n02108551': 'Tibetan',\n",
       " 'n02108915': 'French',\n",
       " 'n02109047': 'Great',\n",
       " 'n02109525': 'Saint',\n",
       " 'n02109961': 'Eskimo',\n",
       " 'n02110063': 'malamute',\n",
       " 'n02110185': 'Siberian',\n",
       " 'n02110341': 'dalmatian',\n",
       " 'n02110627': 'affenpinscher',\n",
       " 'n02110806': 'basenji',\n",
       " 'n02110958': 'pug',\n",
       " 'n02111129': 'Leonberg',\n",
       " 'n02111277': 'Newfoundland',\n",
       " 'n02111500': 'Great',\n",
       " 'n02111889': 'Samoyed',\n",
       " 'n02112018': 'Pomeranian',\n",
       " 'n02112137': 'chow',\n",
       " 'n02112350': 'keeshond',\n",
       " 'n02112706': 'Brabancon',\n",
       " 'n02113023': 'Pembroke',\n",
       " 'n02113186': 'Cardigan',\n",
       " 'n02113624': 'toy',\n",
       " 'n02113712': 'miniature',\n",
       " 'n02113799': 'standard',\n",
       " 'n02113978': 'Mexican',\n",
       " 'n02114367': 'timber',\n",
       " 'n02114548': 'white',\n",
       " 'n02114712': 'red',\n",
       " 'n02114855': 'coyote',\n",
       " 'n02115641': 'dingo',\n",
       " 'n02115913': 'dhole',\n",
       " 'n02116738': 'African',\n",
       " 'n02117135': 'hyena',\n",
       " 'n02119022': 'red',\n",
       " 'n02119789': 'kit',\n",
       " 'n02120079': 'Arctic',\n",
       " 'n02120505': 'grey',\n",
       " 'n02123045': 'tabby',\n",
       " 'n02123159': 'tiger',\n",
       " 'n02123394': 'Persian',\n",
       " 'n02123597': 'Siamese',\n",
       " 'n02124075': 'Egyptian',\n",
       " 'n02125311': 'cougar',\n",
       " 'n02127052': 'lynx',\n",
       " 'n02128385': 'leopard',\n",
       " 'n02128757': 'snow',\n",
       " 'n02128925': 'jaguar',\n",
       " 'n02129165': 'lion',\n",
       " 'n02129604': 'tiger',\n",
       " 'n02130308': 'cheetah',\n",
       " 'n02132136': 'brown',\n",
       " 'n02133161': 'American',\n",
       " 'n02134084': 'ice',\n",
       " 'n02134418': 'sloth',\n",
       " 'n02137549': 'mongoose',\n",
       " 'n02138441': 'meerkat',\n",
       " 'n02165105': 'tiger',\n",
       " 'n02165456': 'ladybug',\n",
       " 'n02167151': 'ground',\n",
       " 'n02168699': 'longhorned',\n",
       " 'n02169497': 'leaf',\n",
       " 'n02172182': 'dung',\n",
       " 'n02174001': 'rhinoceros',\n",
       " 'n02177972': 'weevil',\n",
       " 'n02190166': 'fly',\n",
       " 'n02206856': 'bee',\n",
       " 'n02219486': 'ant',\n",
       " 'n02226429': 'grasshopper',\n",
       " 'n02229544': 'cricket',\n",
       " 'n02231487': 'walking',\n",
       " 'n02233338': 'cockroach',\n",
       " 'n02236044': 'mantis',\n",
       " 'n02256656': 'cicada',\n",
       " 'n02259212': 'leafhopper',\n",
       " 'n02264363': 'lacewing',\n",
       " 'n02268443': 'dragonfly',\n",
       " 'n02268853': 'damselfly',\n",
       " 'n02276258': 'admiral',\n",
       " 'n02277742': 'ringlet',\n",
       " 'n02279972': 'monarch',\n",
       " 'n02280649': 'cabbage',\n",
       " 'n02281406': 'sulphur',\n",
       " 'n02281787': 'lycaenid',\n",
       " 'n02317335': 'starfish',\n",
       " 'n02319095': 'sea',\n",
       " 'n02321529': 'sea',\n",
       " 'n02325366': 'wood',\n",
       " 'n02326432': 'hare',\n",
       " 'n02328150': 'Angora',\n",
       " 'n02342885': 'hamster',\n",
       " 'n02346627': 'porcupine',\n",
       " 'n02356798': 'fox',\n",
       " 'n02361337': 'marmot',\n",
       " 'n02363005': 'beaver',\n",
       " 'n02364673': 'guinea',\n",
       " 'n02389026': 'sorrel',\n",
       " 'n02391049': 'zebra',\n",
       " 'n02395406': 'hog',\n",
       " 'n02396427': 'wild',\n",
       " 'n02397096': 'warthog',\n",
       " 'n02398521': 'hippopotamus',\n",
       " 'n02403003': 'ox',\n",
       " 'n02408429': 'water',\n",
       " 'n02410509': 'bison',\n",
       " 'n02412080': 'ram',\n",
       " 'n02415577': 'bighorn',\n",
       " 'n02417914': 'ibex',\n",
       " 'n02422106': 'hartebeest',\n",
       " 'n02422699': 'impala',\n",
       " 'n02423022': 'gazelle',\n",
       " 'n02437312': 'Arabian',\n",
       " 'n02437616': 'llama',\n",
       " 'n02441942': 'weasel',\n",
       " 'n02442845': 'mink',\n",
       " 'n02443114': 'polecat',\n",
       " 'n02443484': 'blackfooted',\n",
       " 'n02444819': 'otter',\n",
       " 'n02445715': 'skunk',\n",
       " 'n02447366': 'badger',\n",
       " 'n02454379': 'armadillo',\n",
       " 'n02457408': 'threetoed',\n",
       " 'n02480495': 'orangutan',\n",
       " 'n02480855': 'gorilla',\n",
       " 'n02481823': 'chimpanzee',\n",
       " 'n02483362': 'gibbon',\n",
       " 'n02483708': 'siamang',\n",
       " 'n02484975': 'guenon',\n",
       " 'n02486261': 'patas',\n",
       " 'n02486410': 'baboon',\n",
       " 'n02487347': 'macaque',\n",
       " 'n02488291': 'langur',\n",
       " 'n02488702': 'colobus',\n",
       " 'n02489166': 'proboscis',\n",
       " 'n02490219': 'marmoset',\n",
       " 'n02492035': 'capuchin',\n",
       " 'n02492660': 'howler',\n",
       " 'n02493509': 'titi',\n",
       " 'n02493793': 'spider',\n",
       " 'n02494079': 'squirrel',\n",
       " 'n02497673': 'Madagascar',\n",
       " 'n02500267': 'indri',\n",
       " 'n02504013': 'Indian',\n",
       " 'n02504458': 'African',\n",
       " 'n02509815': 'lesser',\n",
       " 'n02510455': 'giant',\n",
       " 'n02514041': 'barracouta',\n",
       " 'n02526121': 'eel',\n",
       " 'n02536864': 'coho',\n",
       " 'n02606052': 'rock',\n",
       " 'n02607072': 'anemone',\n",
       " 'n02640242': 'sturgeon',\n",
       " 'n02641379': 'gar',\n",
       " 'n02643566': 'lionfish',\n",
       " 'n02655020': 'puffer',\n",
       " 'n02666196': 'abacus',\n",
       " 'n02667093': 'abaya',\n",
       " 'n02669723': 'academic',\n",
       " 'n02672831': 'accordion',\n",
       " 'n02676566': 'acoustic',\n",
       " 'n02687172': 'aircraft',\n",
       " 'n02690373': 'airliner',\n",
       " 'n02692877': 'airship',\n",
       " 'n02699494': 'altar',\n",
       " 'n02701002': 'ambulance',\n",
       " 'n02704792': 'amphibian',\n",
       " 'n02708093': 'analog',\n",
       " 'n02727426': 'apiary',\n",
       " 'n02730930': 'apron',\n",
       " 'n02747177': 'ashcan',\n",
       " 'n02749479': 'assault',\n",
       " 'n02769748': 'backpack',\n",
       " 'n02776631': 'bakery',\n",
       " 'n02777292': 'balance',\n",
       " 'n02782093': 'balloon',\n",
       " 'n02783161': 'ballpoint',\n",
       " 'n02786058': 'Band',\n",
       " 'n02787622': 'banjo',\n",
       " 'n02788148': 'bannister',\n",
       " 'n02790996': 'barbell',\n",
       " 'n02791124': 'barber',\n",
       " 'n02791270': 'barbershop',\n",
       " 'n02793495': 'barn',\n",
       " 'n02794156': 'barometer',\n",
       " 'n02795169': 'barrel',\n",
       " 'n02797295': 'barrow',\n",
       " 'n02799071': 'baseball',\n",
       " 'n02802426': 'basketball',\n",
       " 'n02804414': 'bassinet',\n",
       " 'n02804610': 'bassoon',\n",
       " 'n02807133': 'bathing',\n",
       " 'n02808304': 'bath',\n",
       " 'n02808440': 'bathtub',\n",
       " 'n02814533': 'beach',\n",
       " 'n02814860': 'beacon',\n",
       " 'n02815834': 'beaker',\n",
       " 'n02817516': 'bearskin',\n",
       " 'n02823428': 'beer',\n",
       " 'n02823750': 'beer',\n",
       " 'n02825657': 'bell',\n",
       " 'n02834397': 'bib',\n",
       " 'n02835271': 'bicyclebuiltfortwo',\n",
       " 'n02837789': 'bikini',\n",
       " 'n02840245': 'binder',\n",
       " 'n02841315': 'binoculars',\n",
       " 'n02843684': 'birdhouse',\n",
       " 'n02859443': 'boathouse',\n",
       " 'n02860847': 'bobsled',\n",
       " 'n02865351': 'bolo',\n",
       " 'n02869837': 'bonnet',\n",
       " 'n02870880': 'bookcase',\n",
       " 'n02871525': 'bookshop',\n",
       " 'n02877765': 'bottlecap',\n",
       " 'n02879718': 'bow',\n",
       " 'n02883205': 'bow',\n",
       " 'n02892201': 'brass',\n",
       " 'n02892767': 'brassiere',\n",
       " 'n02894605': 'breakwater',\n",
       " 'n02895154': 'breastplate',\n",
       " 'n02906734': 'broom',\n",
       " 'n02909870': 'bucket',\n",
       " 'n02910353': 'buckle',\n",
       " 'n02916936': 'bulletproof',\n",
       " 'n02917067': 'bullet',\n",
       " 'n02927161': 'butcher',\n",
       " 'n02930766': 'cab',\n",
       " 'n02939185': 'caldron',\n",
       " 'n02948072': 'candle',\n",
       " 'n02950826': 'cannon',\n",
       " 'n02951358': 'canoe',\n",
       " 'n02951585': 'can',\n",
       " 'n02963159': 'cardigan',\n",
       " 'n02965783': 'car',\n",
       " 'n02966193': 'carousel',\n",
       " 'n02966687': 'carpenters',\n",
       " 'n02971356': 'carton',\n",
       " 'n02974003': 'car',\n",
       " 'n02977058': 'cash',\n",
       " 'n02978881': 'cassette',\n",
       " 'n02979186': 'cassette',\n",
       " 'n02980441': 'castle',\n",
       " 'n02981792': 'catamaran',\n",
       " 'n02988304': 'CD',\n",
       " 'n02992211': 'cello',\n",
       " 'n02992529': 'cellular',\n",
       " 'n02999410': 'chain',\n",
       " 'n03000134': 'chainlink',\n",
       " 'n03000247': 'chain',\n",
       " 'n03000684': 'chain',\n",
       " 'n03014705': 'chest',\n",
       " 'n03016953': 'chiffonier',\n",
       " 'n03017168': 'chime',\n",
       " 'n03018349': 'china',\n",
       " 'n03026506': 'Christmas',\n",
       " 'n03028079': 'church',\n",
       " 'n03032252': 'cinema',\n",
       " 'n03041632': 'cleaver',\n",
       " 'n03042490': 'cliff',\n",
       " 'n03045698': 'cloak',\n",
       " 'n03047690': 'clog',\n",
       " 'n03062245': 'cocktail',\n",
       " 'n03063599': 'coffee',\n",
       " 'n03063689': 'coffeepot',\n",
       " 'n03065424': 'coil',\n",
       " 'n03075370': 'combination',\n",
       " 'n03085013': 'computer',\n",
       " 'n03089624': 'confectionery',\n",
       " 'n03095699': 'container',\n",
       " 'n03100240': 'convertible',\n",
       " 'n03109150': 'corkscrew',\n",
       " 'n03110669': 'cornet',\n",
       " 'n03124043': 'cowboy',\n",
       " 'n03124170': 'cowboy',\n",
       " 'n03125729': 'cradle',\n",
       " 'n03126707': 'crane',\n",
       " 'n03127747': 'crash',\n",
       " 'n03127925': 'crate',\n",
       " 'n03131574': 'crib',\n",
       " 'n03133878': 'Crock',\n",
       " 'n03134739': 'croquet',\n",
       " 'n03141823': 'crutch',\n",
       " 'n03146219': 'cuirass',\n",
       " 'n03160309': 'dam',\n",
       " 'n03179701': 'desk',\n",
       " 'n03180011': 'desktop',\n",
       " 'n03187595': 'dial',\n",
       " 'n03188531': 'diaper',\n",
       " 'n03196217': 'digital',\n",
       " 'n03197337': 'digital',\n",
       " 'n03201208': 'dining',\n",
       " 'n03207743': 'dishrag',\n",
       " 'n03207941': 'dishwasher',\n",
       " 'n03208938': 'disk',\n",
       " 'n03216828': 'dock',\n",
       " 'n03218198': 'dogsled',\n",
       " 'n03220513': 'dome',\n",
       " 'n03223299': 'doormat',\n",
       " 'n03240683': 'drilling',\n",
       " 'n03249569': 'drum',\n",
       " 'n03250847': 'drumstick',\n",
       " 'n03255030': 'dumbbell',\n",
       " 'n03259280': 'Dutch',\n",
       " 'n03271574': 'electric',\n",
       " 'n03272010': 'electric',\n",
       " 'n03272562': 'electric',\n",
       " 'n03290653': 'entertainment',\n",
       " 'n03291819': 'envelope',\n",
       " 'n03297495': 'espresso',\n",
       " 'n03314780': 'face',\n",
       " 'n03325584': 'feather',\n",
       " 'n03337140': 'file',\n",
       " 'n03344393': 'fireboat',\n",
       " 'n03345487': 'fire',\n",
       " 'n03347037': 'fire',\n",
       " 'n03355925': 'flagpole',\n",
       " 'n03372029': 'flute',\n",
       " 'n03376595': 'folding',\n",
       " 'n03379051': 'football',\n",
       " 'n03384352': 'forklift',\n",
       " 'n03388043': 'fountain',\n",
       " 'n03388183': 'fountain',\n",
       " 'n03388549': 'fourposter',\n",
       " 'n03393912': 'freight',\n",
       " 'n03394916': 'French',\n",
       " 'n03400231': 'frying',\n",
       " 'n03404251': 'fur',\n",
       " 'n03417042': 'garbage',\n",
       " 'n03424325': 'gasmask',\n",
       " 'n03425413': 'gas',\n",
       " 'n03443371': 'goblet',\n",
       " 'n03444034': 'gokart',\n",
       " 'n03445777': 'golf',\n",
       " 'n03445924': 'golfcart',\n",
       " 'n03447447': 'gondola',\n",
       " 'n03447721': 'gong',\n",
       " 'n03450230': 'gown',\n",
       " 'n03452741': 'grand',\n",
       " 'n03457902': 'greenhouse',\n",
       " 'n03459775': 'grille',\n",
       " 'n03461385': 'grocery',\n",
       " 'n03467068': 'guillotine',\n",
       " 'n03476684': 'hair',\n",
       " 'n03476991': 'hair',\n",
       " 'n03478589': 'half',\n",
       " 'n03481172': 'hammer',\n",
       " 'n03482405': 'hamper',\n",
       " 'n03483316': 'hand',\n",
       " 'n03485407': 'handheld',\n",
       " 'n03485794': 'handkerchief',\n",
       " 'n03492542': 'hard',\n",
       " 'n03494278': 'harmonica',\n",
       " 'n03495258': 'harp',\n",
       " 'n03496892': 'harvester',\n",
       " 'n03498962': 'hatchet',\n",
       " 'n03527444': 'holster',\n",
       " 'n03529860': 'home',\n",
       " 'n03530642': 'honeycomb',\n",
       " 'n03532672': 'hook',\n",
       " 'n03534580': 'hoopskirt',\n",
       " 'n03535780': 'horizontal',\n",
       " 'n03538406': 'horse',\n",
       " 'n03544143': 'hourglass',\n",
       " 'n03584254': 'iPod',\n",
       " 'n03584829': 'iron',\n",
       " 'n03590841': 'jackolantern',\n",
       " 'n03594734': 'jean',\n",
       " 'n03594945': 'jeep',\n",
       " 'n03595614': 'jersey',\n",
       " 'n03598930': 'jigsaw',\n",
       " 'n03599486': 'jinrikisha',\n",
       " 'n03602883': 'joystick',\n",
       " 'n03617480': 'kimono',\n",
       " 'n03623198': 'knee',\n",
       " 'n03627232': 'knot',\n",
       " 'n03630383': 'lab',\n",
       " 'n03633091': 'ladle',\n",
       " 'n03637318': 'lampshade',\n",
       " 'n03642806': 'laptop',\n",
       " 'n03649909': 'lawn',\n",
       " 'n03657121': 'lens',\n",
       " 'n03658185': 'letter',\n",
       " 'n03661043': 'library',\n",
       " 'n03662601': 'lifeboat',\n",
       " 'n03666591': 'lighter',\n",
       " 'n03670208': 'limousine',\n",
       " 'n03673027': 'liner',\n",
       " 'n03676483': 'lipstick',\n",
       " 'n03680355': 'Loafer',\n",
       " 'n03690938': 'lotion',\n",
       " 'n03691459': 'loudspeaker',\n",
       " 'n03692522': 'loupe',\n",
       " 'n03697007': 'lumbermill',\n",
       " 'n03706229': 'magnetic',\n",
       " 'n03709823': 'mailbag',\n",
       " 'n03710193': 'mailbox',\n",
       " 'n03710637': 'maillot',\n",
       " 'n03710721': 'maillot',\n",
       " 'n03717622': 'manhole',\n",
       " 'n03720891': 'maraca',\n",
       " 'n03721384': 'marimba',\n",
       " 'n03724870': 'mask',\n",
       " 'n03729826': 'matchstick',\n",
       " 'n03733131': 'maypole',\n",
       " 'n03733281': 'maze',\n",
       " 'n03733805': 'measuring',\n",
       " 'n03742115': 'medicine',\n",
       " 'n03743016': 'megalith',\n",
       " 'n03759954': 'microphone',\n",
       " 'n03761084': 'microwave',\n",
       " 'n03763968': 'military',\n",
       " 'n03764736': 'milk',\n",
       " 'n03769881': 'minibus',\n",
       " 'n03770439': 'miniskirt',\n",
       " 'n03770679': 'minivan',\n",
       " 'n03773504': 'missile',\n",
       " 'n03775071': 'mitten',\n",
       " 'n03775546': 'mixing',\n",
       " 'n03776460': 'mobile',\n",
       " 'n03777568': 'Model',\n",
       " 'n03777754': 'modem',\n",
       " 'n03781244': 'monastery',\n",
       " 'n03782006': 'monitor',\n",
       " 'n03785016': 'moped',\n",
       " 'n03786901': 'mortar',\n",
       " 'n03787032': 'mortarboard',\n",
       " 'n03788195': 'mosque',\n",
       " 'n03788365': 'mosquito',\n",
       " 'n03791053': 'motor',\n",
       " 'n03792782': 'mountain',\n",
       " 'n03792972': 'mountain',\n",
       " 'n03793489': 'mouse',\n",
       " 'n03794056': 'mousetrap',\n",
       " 'n03796401': 'moving',\n",
       " 'n03803284': 'muzzle',\n",
       " 'n03804744': 'nail',\n",
       " 'n03814639': 'neck',\n",
       " 'n03814906': 'necklace',\n",
       " 'n03825788': 'nipple',\n",
       " 'n03832673': 'notebook',\n",
       " 'n03837869': 'obelisk',\n",
       " 'n03838899': 'oboe',\n",
       " 'n03840681': 'ocarina',\n",
       " 'n03841143': 'odometer',\n",
       " 'n03843555': 'oil',\n",
       " 'n03854065': 'organ',\n",
       " 'n03857828': 'oscilloscope',\n",
       " 'n03866082': 'overskirt',\n",
       " 'n03868242': 'oxcart',\n",
       " 'n03868863': 'oxygen',\n",
       " 'n03871628': 'packet',\n",
       " 'n03873416': 'paddle',\n",
       " 'n03874293': 'paddlewheel',\n",
       " 'n03874599': 'padlock',\n",
       " 'n03876231': 'paintbrush',\n",
       " 'n03877472': 'pajama',\n",
       " 'n03877845': 'palace',\n",
       " 'n03884397': 'panpipe',\n",
       " 'n03887697': 'paper',\n",
       " 'n03888257': 'parachute',\n",
       " 'n03888605': 'parallel',\n",
       " 'n03891251': 'park',\n",
       " 'n03891332': 'parking',\n",
       " 'n03895866': 'passenger',\n",
       " 'n03899768': 'patio',\n",
       " 'n03902125': 'payphone',\n",
       " 'n03903868': 'pedestal',\n",
       " 'n03908618': 'pencil',\n",
       " 'n03908714': 'pencil',\n",
       " 'n03916031': 'perfume',\n",
       " 'n03920288': 'Petri',\n",
       " 'n03924679': 'photocopier',\n",
       " 'n03929660': 'pick',\n",
       " 'n03929855': 'pickelhaube',\n",
       " 'n03930313': 'picket',\n",
       " 'n03930630': 'pickup',\n",
       " 'n03933933': 'pier',\n",
       " 'n03935335': 'piggy',\n",
       " 'n03937543': 'pill',\n",
       " 'n03938244': 'pillow',\n",
       " 'n03942813': 'pingpong',\n",
       " 'n03944341': 'pinwheel',\n",
       " 'n03947888': 'pirate',\n",
       " 'n03950228': 'pitcher',\n",
       " 'n03954731': 'plane',\n",
       " 'n03956157': 'planetarium',\n",
       " 'n03958227': 'plastic',\n",
       " 'n03961711': 'plate',\n",
       " 'n03967562': 'plow',\n",
       " 'n03970156': 'plunger',\n",
       " 'n03976467': 'Polaroid',\n",
       " 'n03976657': 'pole',\n",
       " 'n03977966': 'police',\n",
       " 'n03980874': 'poncho',\n",
       " 'n03982430': 'pool',\n",
       " 'n03983396': 'pop',\n",
       " 'n03991062': 'pot',\n",
       " 'n03992509': 'potters',\n",
       " 'n03995372': 'power',\n",
       " 'n03998194': 'prayer',\n",
       " 'n04004767': 'printer',\n",
       " 'n04005630': 'prison',\n",
       " 'n04008634': 'projectile',\n",
       " 'n04009552': 'projector',\n",
       " 'n04019541': 'puck',\n",
       " 'n04023962': 'punching',\n",
       " 'n04026417': 'purse',\n",
       " 'n04033901': 'quill',\n",
       " 'n04033995': 'quilt',\n",
       " 'n04037443': 'racer',\n",
       " 'n04039381': 'racket',\n",
       " 'n04040759': 'radiator',\n",
       " 'n04041544': 'radio',\n",
       " 'n04044716': 'radio',\n",
       " 'n04049303': 'rain',\n",
       " 'n04065272': 'recreational',\n",
       " 'n04067472': 'reel',\n",
       " 'n04069434': 'reflex',\n",
       " 'n04070727': 'refrigerator',\n",
       " 'n04074963': 'remote',\n",
       " 'n04081281': 'restaurant',\n",
       " 'n04086273': 'revolver',\n",
       " 'n04090263': 'rifle',\n",
       " 'n04099969': 'rocking',\n",
       " 'n04111531': 'rotisserie',\n",
       " 'n04116512': 'rubber',\n",
       " 'n04118538': 'rugby',\n",
       " 'n04118776': 'rule',\n",
       " 'n04120489': 'running',\n",
       " 'n04125021': 'safe',\n",
       " 'n04127249': 'safety',\n",
       " 'n04131690': 'saltshaker',\n",
       " 'n04133789': 'sandal',\n",
       " 'n04136333': 'sarong',\n",
       " 'n04141076': 'sax',\n",
       " 'n04141327': 'scabbard',\n",
       " 'n04141975': 'scale',\n",
       " 'n04146614': 'school',\n",
       " 'n04147183': 'schooner',\n",
       " 'n04149813': 'scoreboard',\n",
       " 'n04152593': 'screen',\n",
       " 'n04153751': 'screw',\n",
       " 'n04154565': 'screwdriver',\n",
       " 'n04162706': 'seat',\n",
       " 'n04179913': 'sewing',\n",
       " 'n04192698': 'shield',\n",
       " 'n04200800': 'shoe',\n",
       " 'n04201297': 'shoji',\n",
       " 'n04204238': 'shopping',\n",
       " 'n04204347': 'shopping',\n",
       " 'n04208210': 'shovel',\n",
       " 'n04209133': 'shower',\n",
       " 'n04209239': 'shower',\n",
       " 'n04228054': 'ski',\n",
       " 'n04229816': 'ski',\n",
       " 'n04235860': 'sleeping',\n",
       " 'n04238763': 'slide',\n",
       " 'n04239074': 'sliding',\n",
       " 'n04243546': 'slot',\n",
       " 'n04251144': 'snorkel',\n",
       " 'n04252077': 'snowmobile',\n",
       " 'n04252225': 'snowplow',\n",
       " 'n04254120': 'soap',\n",
       " 'n04254680': 'soccer',\n",
       " 'n04254777': 'sock',\n",
       " 'n04258138': 'solar',\n",
       " 'n04259630': 'sombrero',\n",
       " 'n04263257': 'soup',\n",
       " 'n04264628': 'space',\n",
       " 'n04265275': 'space',\n",
       " 'n04266014': 'space',\n",
       " 'n04270147': 'spatula',\n",
       " 'n04273569': 'speedboat',\n",
       " 'n04275548': 'spider',\n",
       " 'n04277352': 'spindle',\n",
       " 'n04285008': 'sports',\n",
       " 'n04286575': 'spotlight',\n",
       " 'n04296562': 'stage',\n",
       " 'n04310018': 'steam',\n",
       " 'n04311004': 'steel',\n",
       " 'n04311174': 'steel',\n",
       " 'n04317175': 'stethoscope',\n",
       " 'n04325704': 'stole',\n",
       " 'n04326547': 'stone',\n",
       " 'n04328186': 'stopwatch',\n",
       " 'n04330267': 'stove',\n",
       " 'n04332243': 'strainer',\n",
       " 'n04335435': 'streetcar',\n",
       " 'n04336792': 'stretcher',\n",
       " 'n04344873': 'studio',\n",
       " 'n04346328': 'stupa',\n",
       " 'n04347754': 'submarine',\n",
       " 'n04350905': 'suit',\n",
       " 'n04355338': 'sundial',\n",
       " 'n04355933': 'sunglass',\n",
       " 'n04356056': 'sunglasses',\n",
       " 'n04357314': 'sunscreen',\n",
       " 'n04366367': 'suspension',\n",
       " 'n04367480': 'swab',\n",
       " 'n04370456': 'sweatshirt',\n",
       " 'n04371430': 'swimming',\n",
       " 'n04371774': 'swing',\n",
       " 'n04372370': 'switch',\n",
       " 'n04376876': 'syringe',\n",
       " 'n04380533': 'table',\n",
       " 'n04389033': 'tank',\n",
       " 'n04392985': 'tape',\n",
       " 'n04398044': 'teapot',\n",
       " 'n04399382': 'teddy',\n",
       " 'n04404412': 'television',\n",
       " 'n04409515': 'tennis',\n",
       " 'n04417672': 'thatch',\n",
       " 'n04418357': 'theater',\n",
       " 'n04423845': 'thimble',\n",
       " 'n04428191': 'thresher',\n",
       " 'n04429376': 'throne',\n",
       " 'n04435653': 'tile',\n",
       " 'n04442312': 'toaster',\n",
       " 'n04443257': 'tobacco',\n",
       " 'n04447861': 'toilet',\n",
       " 'n04456115': 'torch',\n",
       " 'n04458633': 'totem',\n",
       " 'n04461696': 'tow',\n",
       " 'n04462240': 'toyshop',\n",
       " 'n04465501': 'tractor',\n",
       " 'n04467665': 'trailer',\n",
       " 'n04476259': 'tray',\n",
       " 'n04479046': 'trench',\n",
       " 'n04482393': 'tricycle',\n",
       " 'n04483307': 'trimaran',\n",
       " 'n04485082': 'tripod',\n",
       " 'n04486054': 'triumphal',\n",
       " 'n04487081': 'trolleybus',\n",
       " 'n04487394': 'trombone',\n",
       " 'n04493381': 'tub',\n",
       " 'n04501370': 'turnstile',\n",
       " 'n04505470': 'typewriter',\n",
       " 'n04507155': 'umbrella',\n",
       " 'n04509417': 'unicycle',\n",
       " 'n04515003': 'upright',\n",
       " 'n04517823': 'vacuum',\n",
       " 'n04522168': 'vase',\n",
       " 'n04523525': 'vault',\n",
       " 'n04525038': 'velvet',\n",
       " 'n04525305': 'vending',\n",
       " 'n04532106': 'vestment',\n",
       " 'n04532670': 'viaduct',\n",
       " 'n04536866': 'violin',\n",
       " 'n04540053': 'volleyball',\n",
       " 'n04542943': 'waffle',\n",
       " 'n04548280': 'wall',\n",
       " 'n04548362': 'wallet',\n",
       " 'n04550184': 'wardrobe',\n",
       " 'n04552348': 'warplane',\n",
       " 'n04553703': 'washbasin',\n",
       " 'n04554684': 'washer',\n",
       " 'n04557648': 'water',\n",
       " 'n04560804': 'water',\n",
       " 'n04562935': 'water',\n",
       " 'n04579145': 'whiskey',\n",
       " 'n04579432': 'whistle',\n",
       " 'n04584207': 'wig',\n",
       " 'n04589890': 'window',\n",
       " 'n04590129': 'window',\n",
       " 'n04591157': 'Windsor',\n",
       " 'n04591713': 'wine',\n",
       " 'n04592741': 'wing',\n",
       " 'n04596742': 'wok',\n",
       " 'n04597913': 'wooden',\n",
       " 'n04599235': 'wool',\n",
       " 'n04604644': 'worm',\n",
       " 'n04606251': 'wreck',\n",
       " 'n04612504': 'yawl',\n",
       " 'n04613696': 'yurt',\n",
       " 'n06359193': 'web',\n",
       " 'n06596364': 'comic',\n",
       " 'n06785654': 'crossword',\n",
       " 'n06794110': 'street',\n",
       " 'n06874185': 'traffic',\n",
       " 'n07248320': 'book',\n",
       " 'n07565083': 'menu',\n",
       " 'n07579787': 'plate',\n",
       " 'n07583066': 'guacamole',\n",
       " 'n07584110': 'consomme',\n",
       " 'n07590611': 'hot',\n",
       " 'n07613480': 'trifle',\n",
       " 'n07614500': 'ice',\n",
       " 'n07615774': 'ice',\n",
       " 'n07684084': 'French',\n",
       " 'n07693725': 'bagel',\n",
       " 'n07695742': 'pretzel',\n",
       " 'n07697313': 'cheeseburger',\n",
       " 'n07697537': 'hotdog',\n",
       " 'n07711569': 'mashed',\n",
       " 'n07714571': 'head',\n",
       " 'n07714990': 'broccoli',\n",
       " 'n07715103': 'cauliflower',\n",
       " 'n07716358': 'zucchini',\n",
       " 'n07716906': 'spaghetti',\n",
       " 'n07717410': 'acorn',\n",
       " 'n07717556': 'butternut',\n",
       " 'n07718472': 'cucumber',\n",
       " 'n07718747': 'artichoke',\n",
       " 'n07720875': 'bell',\n",
       " 'n07730033': 'cardoon',\n",
       " 'n07734744': 'mushroom',\n",
       " 'n07742313': 'Granny',\n",
       " 'n07745940': 'strawberry',\n",
       " 'n07747607': 'orange',\n",
       " 'n07749582': 'lemon',\n",
       " 'n07753113': 'fig',\n",
       " 'n07753275': 'pineapple',\n",
       " 'n07753592': 'banana',\n",
       " 'n07754684': 'jackfruit',\n",
       " 'n07760859': 'custard',\n",
       " 'n07768694': 'pomegranate',\n",
       " 'n07802026': 'hay',\n",
       " 'n07831146': 'carbonara',\n",
       " 'n07836838': 'chocolate',\n",
       " 'n07860988': 'dough',\n",
       " 'n07871810': 'meat',\n",
       " 'n07873807': 'pizza',\n",
       " 'n07875152': 'potpie',\n",
       " 'n07880968': 'burrito',\n",
       " 'n07892512': 'red',\n",
       " 'n07920052': 'espresso',\n",
       " 'n07930864': 'cup',\n",
       " 'n07932039': 'eggnog',\n",
       " 'n09193705': 'alp',\n",
       " 'n09229709': 'bubble',\n",
       " 'n09246464': 'cliff',\n",
       " 'n09256479': 'coral',\n",
       " 'n09288635': 'geyser',\n",
       " 'n09332890': 'lakeside',\n",
       " 'n09399592': 'promontory',\n",
       " 'n09421951': 'sandbar',\n",
       " 'n09428293': 'seashore',\n",
       " 'n09468604': 'valley',\n",
       " 'n09472597': 'volcano',\n",
       " 'n09835506': 'ballplayer',\n",
       " 'n10148035': 'groom',\n",
       " 'n10565667': 'scuba',\n",
       " 'n11879895': 'rapeseed',\n",
       " 'n11939491': 'daisy',\n",
       " 'n12057211': 'yellow',\n",
       " 'n12144580': 'corn',\n",
       " 'n12267677': 'acorn',\n",
       " 'n12620546': 'hip',\n",
       " 'n12768682': 'buckeye',\n",
       " 'n12985857': 'coral',\n",
       " 'n12998815': 'agaric',\n",
       " 'n13037406': 'gyromitra',\n",
       " 'n13040303': 'stinkhorn',\n",
       " 'n13044778': 'earthstar',\n",
       " 'n13052670': 'henofthewoods',\n",
       " 'n13054560': 'bolete',\n",
       " 'n13133613': 'ear',\n",
       " 'n15075141': 'toilet'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_path = label_path / 'synset_words.txt'\n",
    "\n",
    "def synset2word(synset_path=synset_path):\n",
    "    label_dict = {}\n",
    "    with open(synset_path, 'r') as f:\n",
    "        synset_word = f.readlines()\n",
    "        for i in range(len(synset_word)):\n",
    "            synset = synset_word[i].split()[0]\n",
    "            word = re.sub(r'[^a-zA-Z]', '', synset_word[i].split()[1])\n",
    "            label_dict[synset] = word\n",
    "            \n",
    "    return label_dict\n",
    "\n",
    "label_dict = synset2word()\n",
    "label_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, local_rank=0, no_save=False):\n",
    "        self.terminal = sys.stdout\n",
    "        self.file = None\n",
    "        self.local_rank = local_rank\n",
    "        self.no_save = no_save\n",
    "    def open(self, fp, mode=None):\n",
    "        if mode is None: mode = 'w'\n",
    "        if self.local_rank and not self.no_save == 0: self.file = open(fp, mode)\n",
    "    def write(self, msg, is_terminal=1, is_file=1):\n",
    "        if msg[-1] != \"\\n\": msg = msg + \"\\n\"\n",
    "        if self.local_rank == 0:\n",
    "            if '\\r' in msg: is_file = 0\n",
    "            if is_terminal == 1:\n",
    "                self.terminal.write(msg)\n",
    "                self.terminal.flush()\n",
    "            if is_file == 1 and not self.no_save:\n",
    "                self.file.write(msg)\n",
    "                self.file.flush()\n",
    "    def flush(self): \n",
    "        pass\n",
    "    \n",
    "def print_args(args, logger=None):\n",
    "    if logger is not None:\n",
    "        logger.write(\"#### configurations ####\")\n",
    "    for k, v in vars(args).items():\n",
    "        if logger is not None:\n",
    "            logger.write('{}: {}\\n'.format(k, v))\n",
    "        else:\n",
    "            print('{}: {}'.format(k, v))\n",
    "    if logger is not None:\n",
    "        logger.write(\"########################\")\n",
    "      \n",
    "import argparse\n",
    "import json\n",
    "def save_args(args, to_path):\n",
    "    with open(to_path, \"w\") as f:\n",
    "        json.dump(args.__dict__, f, indent=2)\n",
    "def load_args(from_path):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args()\n",
    "    with open(from_path, \"r\") as f:\n",
    "        args.__dict__ = json.load(f)\n",
    "    return args    \n",
    "\n",
    "class AverageMeter (object):\n",
    "    def __init__(self):\n",
    "        self.reset ()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def Accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       BottleNeck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       BottleNeck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       BottleNeck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       BottleNeck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       BottleNeck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       BottleNeck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       BottleNeck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       BottleNeck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      BottleNeck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      BottleNeck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      BottleNeck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      BottleNeck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      BottleNeck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      BottleNeck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      BottleNeck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      BottleNeck-172           [-1, 2048, 7, 7]               0\n",
      "       AvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        \n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * self.expansion),\n",
    "        )\n",
    "        self.expansion = 4\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 하위 layer로 내려갈때 downsampling(1/2줄이기)하면서 feature 수가 달라짐 그러므로 skip connection을 할 수 있게 보정\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = self.residual_function(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        return self.relu(out)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block=BottleNeck, layers=[3,4,6,3], in_channels=3, num_classes=1000):\n",
    "        self.num_classes = num_classes\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            \n",
    "        )        \n",
    "        self.conv2 = self._make_layer(block, 64, layers[0])\n",
    "        self.conv3 = self._make_layer(block, 128, layers[1], downsampling=True)\n",
    "        self.conv4 = self._make_layer(block, 256, layers[2], downsampling=True)\n",
    "        self.conv5 = self._make_layer(block, 512, layers[3], downsampling=True)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _make_layer(self, block, out_channels, num_blocks, downsampling=False):\n",
    "        if downsampling is True:\n",
    "            stride = 2\n",
    "        else:\n",
    "            stride = 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "import torchsummary\n",
    "model = ResNet()\n",
    "torchsummary.summary(model, (3,224,224), device='cpu')\n",
    "# print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1             [-1, 64, 2352]               0\n",
      "            Linear-2             [-1, 64, 1024]       2,409,472\n",
      "           Dropout-3             [-1, 65, 1024]               0\n",
      "         LayerNorm-4             [-1, 65, 1024]           2,048\n",
      "            Linear-5             [-1, 65, 3072]       3,145,728\n",
      "           Softmax-6           [-1, 16, 65, 65]               0\n",
      "           Dropout-7           [-1, 16, 65, 65]               0\n",
      "            Linear-8             [-1, 65, 1024]       1,049,600\n",
      "           Dropout-9             [-1, 65, 1024]               0\n",
      "        Attention-10             [-1, 65, 1024]               0\n",
      "          PreNorm-11             [-1, 65, 1024]               0\n",
      "        LayerNorm-12             [-1, 65, 1024]           2,048\n",
      "           Linear-13             [-1, 65, 2048]       2,099,200\n",
      "             GELU-14             [-1, 65, 2048]               0\n",
      "          Dropout-15             [-1, 65, 2048]               0\n",
      "           Linear-16             [-1, 65, 1024]       2,098,176\n",
      "          Dropout-17             [-1, 65, 1024]               0\n",
      "      FeedForward-18             [-1, 65, 1024]               0\n",
      "          PreNorm-19             [-1, 65, 1024]               0\n",
      "        LayerNorm-20             [-1, 65, 1024]           2,048\n",
      "           Linear-21             [-1, 65, 3072]       3,145,728\n",
      "          Softmax-22           [-1, 16, 65, 65]               0\n",
      "          Dropout-23           [-1, 16, 65, 65]               0\n",
      "           Linear-24             [-1, 65, 1024]       1,049,600\n",
      "          Dropout-25             [-1, 65, 1024]               0\n",
      "        Attention-26             [-1, 65, 1024]               0\n",
      "          PreNorm-27             [-1, 65, 1024]               0\n",
      "        LayerNorm-28             [-1, 65, 1024]           2,048\n",
      "           Linear-29             [-1, 65, 2048]       2,099,200\n",
      "             GELU-30             [-1, 65, 2048]               0\n",
      "          Dropout-31             [-1, 65, 2048]               0\n",
      "           Linear-32             [-1, 65, 1024]       2,098,176\n",
      "          Dropout-33             [-1, 65, 1024]               0\n",
      "      FeedForward-34             [-1, 65, 1024]               0\n",
      "          PreNorm-35             [-1, 65, 1024]               0\n",
      "        LayerNorm-36             [-1, 65, 1024]           2,048\n",
      "           Linear-37             [-1, 65, 3072]       3,145,728\n",
      "          Softmax-38           [-1, 16, 65, 65]               0\n",
      "          Dropout-39           [-1, 16, 65, 65]               0\n",
      "           Linear-40             [-1, 65, 1024]       1,049,600\n",
      "          Dropout-41             [-1, 65, 1024]               0\n",
      "        Attention-42             [-1, 65, 1024]               0\n",
      "          PreNorm-43             [-1, 65, 1024]               0\n",
      "        LayerNorm-44             [-1, 65, 1024]           2,048\n",
      "           Linear-45             [-1, 65, 2048]       2,099,200\n",
      "             GELU-46             [-1, 65, 2048]               0\n",
      "          Dropout-47             [-1, 65, 2048]               0\n",
      "           Linear-48             [-1, 65, 1024]       2,098,176\n",
      "          Dropout-49             [-1, 65, 1024]               0\n",
      "      FeedForward-50             [-1, 65, 1024]               0\n",
      "          PreNorm-51             [-1, 65, 1024]               0\n",
      "        LayerNorm-52             [-1, 65, 1024]           2,048\n",
      "           Linear-53             [-1, 65, 3072]       3,145,728\n",
      "          Softmax-54           [-1, 16, 65, 65]               0\n",
      "          Dropout-55           [-1, 16, 65, 65]               0\n",
      "           Linear-56             [-1, 65, 1024]       1,049,600\n",
      "          Dropout-57             [-1, 65, 1024]               0\n",
      "        Attention-58             [-1, 65, 1024]               0\n",
      "          PreNorm-59             [-1, 65, 1024]               0\n",
      "        LayerNorm-60             [-1, 65, 1024]           2,048\n",
      "           Linear-61             [-1, 65, 2048]       2,099,200\n",
      "             GELU-62             [-1, 65, 2048]               0\n",
      "          Dropout-63             [-1, 65, 2048]               0\n",
      "           Linear-64             [-1, 65, 1024]       2,098,176\n",
      "          Dropout-65             [-1, 65, 1024]               0\n",
      "      FeedForward-66             [-1, 65, 1024]               0\n",
      "          PreNorm-67             [-1, 65, 1024]               0\n",
      "        LayerNorm-68             [-1, 65, 1024]           2,048\n",
      "           Linear-69             [-1, 65, 3072]       3,145,728\n",
      "          Softmax-70           [-1, 16, 65, 65]               0\n",
      "          Dropout-71           [-1, 16, 65, 65]               0\n",
      "           Linear-72             [-1, 65, 1024]       1,049,600\n",
      "          Dropout-73             [-1, 65, 1024]               0\n",
      "        Attention-74             [-1, 65, 1024]               0\n",
      "          PreNorm-75             [-1, 65, 1024]               0\n",
      "        LayerNorm-76             [-1, 65, 1024]           2,048\n",
      "           Linear-77             [-1, 65, 2048]       2,099,200\n",
      "             GELU-78             [-1, 65, 2048]               0\n",
      "          Dropout-79             [-1, 65, 2048]               0\n",
      "           Linear-80             [-1, 65, 1024]       2,098,176\n",
      "          Dropout-81             [-1, 65, 1024]               0\n",
      "      FeedForward-82             [-1, 65, 1024]               0\n",
      "          PreNorm-83             [-1, 65, 1024]               0\n",
      "        LayerNorm-84             [-1, 65, 1024]           2,048\n",
      "           Linear-85             [-1, 65, 3072]       3,145,728\n",
      "          Softmax-86           [-1, 16, 65, 65]               0\n",
      "          Dropout-87           [-1, 16, 65, 65]               0\n",
      "           Linear-88             [-1, 65, 1024]       1,049,600\n",
      "          Dropout-89             [-1, 65, 1024]               0\n",
      "        Attention-90             [-1, 65, 1024]               0\n",
      "          PreNorm-91             [-1, 65, 1024]               0\n",
      "        LayerNorm-92             [-1, 65, 1024]           2,048\n",
      "           Linear-93             [-1, 65, 2048]       2,099,200\n",
      "             GELU-94             [-1, 65, 2048]               0\n",
      "          Dropout-95             [-1, 65, 2048]               0\n",
      "           Linear-96             [-1, 65, 1024]       2,098,176\n",
      "          Dropout-97             [-1, 65, 1024]               0\n",
      "      FeedForward-98             [-1, 65, 1024]               0\n",
      "          PreNorm-99             [-1, 65, 1024]               0\n",
      "     Transformer-100             [-1, 65, 1024]               0\n",
      "       LayerNorm-101                 [-1, 1024]           2,048\n",
      "          Linear-102                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 53,817,320\n",
      "Trainable params: 53,817,320\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 66.76\n",
      "Params size (MB): 205.30\n",
      "Estimated Total Size (MB): 272.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "'''\n",
    "rearrange doesn't change number of elements and covers different numpy functions (like transpose, reshape, stack, concatenate, squeeze and expand_dims)\n",
    "rearrange : string으로 transpose, compose, decompose와 같은 shape 변화를 줄 수 있음\n",
    "\n",
    "reduce combines same reordering syntax with reductions (mean, min, max, sum, prod, and any others)\n",
    "reduce : string으로 mean, min, max, sum, prod같은 작업을 할 수 있음.\n",
    "\n",
    "repeat additionally covers repeating and tiling\n",
    "repeat : string으로 repeat할 수 있음\n",
    "'''\n",
    "from einops.layers.torch import Rearrange # torch에서 layer로 사용하게 함\n",
    "\n",
    "def make_tuple(x):\n",
    "    return x if isinstance(x, tuple) else (x, x)\n",
    "\n",
    "class PreNorm(nn.Module): # 모든 블록 전에 먼저 Norm을 하고 attention 수행함.\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim) # 3(Data 갯수)X6(Feature 갯수) 기준으로 LayerNorm(데이터 샘플 단위) 3개 나옴 VS BatchNorm(특성 단위) 6개 나옴\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads # multi head를 내부에서 한 번에 작업하기 위함\n",
    "        project_out = not (heads == 1 and dim_head == dim) # multi head attention 인지 아닌지\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5 # root(dim_head)로 나눠 줄 때 필요\n",
    "        \n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False) # input을 q, k, v로 만들기\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity() # single head일 때는 할 필요 없으므로\n",
    "        \n",
    "    def forward(self, x): # [batch_size, seq_len, dim]\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) # chunk으로 세 덩어리로 나눔\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv) # [batch_size, heads, seq_len, dim_head] X 3\n",
    "        \n",
    "        energy = torch.matmul(q, k.transpose(-1, -2)) * self.scale \n",
    "        \n",
    "        attn = self.attend(energy)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v) # [batch_size, heads, seq_len, dim_head]\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        return self.to_out(out) # [batch_size, seq_len, emb_dim]\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x # skip connection 필요함\n",
    "            x = ff(x) + x\n",
    "        return x # [batch_size, seq_len, emb_dim]\n",
    " \n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dim_head=64, dropout=0., pool='cls', channels=3, emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = make_tuple(image_size)\n",
    "        patch_height, patch_width = make_tuple(patch_size)\n",
    "        \n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image sizes must be divisible by the patch size'\n",
    "        \n",
    "        num_patchs = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_width * patch_height\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        \n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width), # [batch_size, patch_num, patch_dim]\n",
    "            nn.Linear(patch_dim, dim) # [batch_size, patch_num, emb_dim]\n",
    "        )\n",
    "        \n",
    "        self.pos_embedding = nn.Parameter(torch.rand(1, num_patchs + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        \n",
    "        self.pool = pool\n",
    "        # self.to_latent = nn.Identity() BYOL을 사용할거 아니면 필요없음 lucidrain피셜\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim), # 모든 block 전에 LN사용함.\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embedding(x)\n",
    "        batch_size, num_patchs, emb_size = x.shape\n",
    "        \n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=batch_size) # 맨 앞에 붙일 Class 토큰 만들기\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # 시퀀스 맨 앞에 붙이기\n",
    "        x += self.pos_embedding[:, :(num_patchs+1)] # positional embedding 더하기\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # mlp head에 들어갈 때 맨 앞 cls 토큰만 가져갈지 전체를 평균내어 가져갈지 결정하기\n",
    "        x = x[:, 0] if self.pool == 'cls' else reduce(x, 'b n d -> b d', 'mean') # [batch_size, emb_size]\n",
    "        \n",
    "        # x = self.to_latent(x)\n",
    "        \n",
    "        return self.mlp_head(x) # [batch_size, num_classes]\n",
    "          \n",
    "import torchsummary\n",
    "model = ViT(224, 28, 1000, 1024, 6, 16, 2048, dropout=0.1, emb_dropout=0.1)\n",
    "torchsummary.summary(model, (3,224,224), device='cpu')\n",
    "# print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6741, -0.1814,  0.0143],\n",
      "         [ 0.8440, -0.5426,  0.8293],\n",
      "         [ 1.0445, -0.9468,  0.2765],\n",
      "         [ 1.3212, -0.6894,  0.0998],\n",
      "         [-1.1613,  1.6704, -1.1912]]])\n",
      "==================================\n",
      "tensor([[[ 1.4077, -0.8214, -0.5863],\n",
      "         [ 0.7184, -1.4141,  0.6957],\n",
      "         [ 1.1217, -1.3067,  0.1851],\n",
      "         [ 1.3025, -1.1283, -0.1742],\n",
      "         [-0.6959,  1.4142, -0.7182]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NLP Example\n",
    "batch, sentence_length, embedding_dim = 1, 5, 3\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "# Activate module\n",
    "out = layer_norm(embedding)\n",
    "print(embedding)\n",
    "print(\"==================================\")\n",
    "print(out)\n",
    "\n",
    "# Image Example\n",
    "N, C, H, W = 20, 5, 10, 10\n",
    "input = torch.randn(N, C, H, W)\n",
    "# Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n",
    "# as shown in the image below\n",
    "layer_norm = nn.LayerNorm([C, H, W])\n",
    "output = layer_norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vit runed at 2023-02-02 21:49:47'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "('vit' + ' runed at ' + now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadError",
     "evalue": "file could not be opened successfully",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79933/1782455300.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./runs/vit/checkpoint.pth.tar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IMGNET/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1578\u001b[0m                         \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file could not be opened successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\":\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadError\u001b[0m: file could not be opened successfully"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('./runs/vit/checkpoint.pth.tar', 'r') as f:\n",
    "    print(f.getnames())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMGNET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8494cf7075c74e4f28217251c806f8ad3017af770baf78f62e6cb1283caa6097"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
